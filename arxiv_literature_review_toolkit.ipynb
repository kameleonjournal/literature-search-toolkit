{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06b1d3c1",
   "metadata": {},
   "source": [
    "# ArXiv Literature Search Strategy & CSV Generation\n",
    "This notebook allows you to test different keyword strategies and download records using the ArXiv API ()\n",
    "\n",
    "Check the API documentation on: https://info.arxiv.org/help/api/user-manual.html\n",
    "\n",
    "Edit the `groups` and `logic` in the next code cell, then run the subsequent cells to see the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0369f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SECTION: INSTALL AND IMPORT DEPENDENCIES ===\n",
    "# Commented out: pip install feedparser if not present\n",
    "try:\n",
    "    import feedparser\n",
    "    print(\"'feedparser' is already installed.\")\n",
    "except ModuleNotFoundError:\n",
    "    import subprocess, sys\n",
    "    print(\"'feedparser' not found. Installing now...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"feedparser\"])\n",
    "    import feedparser\n",
    "    print(\"'feedparser' has been installed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dccda83",
   "metadata": {},
   "source": [
    "# 1. Setup & Define Your Folders\n",
    "\n",
    "In the below section uncomment (ctrl+/ on PC or command+/ on Mac) the relevant lines to define the csv and summary folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228ec081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SECTION: USER SETUP (PC/Windows) ===\n",
    "# Uncomment below and edit these variables to match your Windows setup\n",
    "# csv_folder = r\"C:\\Users\\YOUR_USERNAME\\Documents\\csvs\\arxiv_csv\"\n",
    "# summary_folder = r\"C:\\Users\\YOUR_USERNAME\\Documents\\csvs\\summaries\"\n",
    "\n",
    "# === SECTION: USER SETUP (Mac) ===\n",
    "# Uncomment below and edit these lines to match your Mac setup\n",
    "csv_folder = r\"/Users/YOUR_USERNAME/Documents/csvs/arxiv_csv\"\n",
    "summary_folder = r\"/Users/YOUR_USERNAME/Documents/csvs/summaries\"\n",
    "\n",
    "# === SECTION: FOLDER CREATION AND CHECK ===\n",
    "import os\n",
    "\n",
    "os.makedirs(csv_folder, exist_ok=True)\n",
    "os.makedirs(summary_folder, exist_ok=True)\n",
    "missing = []\n",
    "if not os.path.isdir(csv_folder):\n",
    "    missing.append(\"CSV folder\")\n",
    "if not os.path.isdir(summary_folder):\n",
    "    missing.append(\"Summary folder\")\n",
    "\n",
    "if missing:\n",
    "    print(f\"⚠️ WARNING: Please check the following: {', '.join(missing)}\")\n",
    "else:\n",
    "    print(\"✅ Output folders are set up and ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb40ed6b",
   "metadata": {},
   "source": [
    "# 2. Test and adjust your keyword strategy\n",
    "\n",
    "The below 4 sections will help test different keyword groups and their combinations.\n",
    "- 2.1 Run to define groups of keywords using AND/OR rules, then define a combination logic\n",
    "- 2.2. Run to see the number of results returned for each keyword group and the combined query\n",
    "- 2.3. Run to see the first 10 titles for each keyword group\n",
    "- 2.4. Run to see the first 10 titles for the combined keyword group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d22ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SECTION: DEFINE KEYWORD GROUPS, LOGIC, AND DATE FILTER ===\n",
    "groups = {\n",
    "    'group1': 'keyword OR keyword',\n",
    "    'group2': 'keyword OR keyword AND keyword',\n",
    "    'excluded': 'keyword or keyword' # exclusion will be applied at the download stage for arXiv API\n",
    "}\n",
    "\n",
    "# Date filter for time set onwards\n",
    "date_filter = 'submittedDate:[201601010000 TO 300001010000]'\n",
    "\n",
    "# Combine logic and add date filter\n",
    "logic = \"({group1}) AND ({group2}) AND {date_filter}\"\n",
    "combined_query = logic.format(**groups, date_filter=date_filter)\n",
    "print(\"Keyword groups, logic, and date filter defined.\")\n",
    "print(\"Combined arXiv query:\", combined_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cc04cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SECTION: RUN ARXIV QUERY AND RETURN TOTAL RESULTS FOR EACH GROUP AND COMBINED QUERY ===\n",
    "import urllib.parse\n",
    "\n",
    "def arxiv_query(query, max_results=1, start=0):\n",
    "    \"\"\"Query arXiv API and return the parsed feed.\"\"\"\n",
    "    import feedparser\n",
    "    base_url = 'http://export.arxiv.org/api/query?'\n",
    "    search_query = urllib.parse.quote(query)\n",
    "    url = f\"{base_url}search_query=all:{search_query}&start={start}&max_results={max_results}\"\n",
    "    print(f\"Querying arXiv: {url}\")\n",
    "    feed = feedparser.parse(url)\n",
    "    return feed\n",
    "\n",
    "# Print total results for each group\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INDIVIDUAL GROUP RESULTS:\")\n",
    "print(\"=\"*50)\n",
    "for name, query in groups.items():\n",
    "    print(f\"\\nGROUP: {name.upper()}\")\n",
    "    feed = arxiv_query(query, max_results=1)  # Only need 1 result to get total count\n",
    "    total_results = feed.feed.get('opensearch_totalresults', 'unknown')\n",
    "    print(f\"Total results for {name}: {total_results}\")\n",
    "\n",
    "# Print total results for combined query\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMBINED LOGIC RESULTS:\")\n",
    "print(\"=\"*50)\n",
    "feed = arxiv_query(combined_query, max_results=1)\n",
    "total_results = feed.feed.get('opensearch_totalresults', 'unknown')\n",
    "print(f\"Total results for combined query: {total_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd04045b",
   "metadata": {},
   "source": [
    "The next block will show the first 10 titles for each keyword group (except the excluded keyword group).\n",
    "\n",
    "Based on this, you can go back and adust your keyword groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28fd501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SECTION: PRINT FIRST 10 TITLES FOR EACH GROUP ===\n",
    "for name, query in groups.items():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"FIRST 10 TITLES FOR GROUP: {name.upper()}\")\n",
    "    print(\"=\"*50)\n",
    "    feed = arxiv_query(query, max_results=10)\n",
    "    for i, entry in enumerate(feed.entries, 1):\n",
    "        # Clean the title before using it in the f-string\n",
    "        clean_title = entry.title.strip().replace('\\n', ' ')\n",
    "        print(f\"{i}. {clean_title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14de2937",
   "metadata": {},
   "source": [
    "The next block will show the first 10 titles for the combined query.\n",
    "\n",
    "Based on the results you can go back and adjust your groups and logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d8febb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SECTION: PRINT FIRST 10 TITLES FOR COMBINED QUERY ===\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FIRST 10 TITLES FOR COMBINED QUERY:\")\n",
    "print(\"=\"*50)\n",
    "feed = arxiv_query(combined_query, max_results=10)\n",
    "for i, entry in enumerate(feed.entries, 1):\n",
    "    clean_title = entry.title.strip().replace('\\n', ' ')\n",
    "    print(f\"{i}. {clean_title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011b84a3",
   "metadata": {},
   "source": [
    "# 2. Export ArXiv Results to CSV\n",
    "\n",
    "The below script will use your combined query to download titles and abstracts and save them to a CSV file, including author name, title, abstract, year and doi. It will also update the summary table to include the total of found and downloaded records, the source the final query and a timestamp for record keeping purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa622120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SECTION: EXPORT ARXIV RESULTS TO CSV (ALL RESULTS, WITH EXCLUSION) ===\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def get_next_csv_name(folder, base_name):\n",
    "    i = 1\n",
    "    while True:\n",
    "        csv_name = f\"{base_name}_v{i}.csv\"\n",
    "        csv_path = os.path.join(folder, csv_name)\n",
    "        if not os.path.exists(csv_path):\n",
    "            return f\"{base_name}_v{i}\", csv_path\n",
    "        i += 1\n",
    "\n",
    "def is_excluded(entry, excluded_terms):\n",
    "    \"\"\"Return True if any excluded term is found in the title or summary.\"\"\"\n",
    "    text = (entry.title + \" \" + entry.summary).lower()\n",
    "    return any(term.lower() in text for term in excluded_terms)\n",
    "\n",
    "def download_arxiv_to_csv_all(query, csv_folder, excluded_terms, max_total=5000, batch_size=100):\n",
    "    import feedparser\n",
    "    os.makedirs(csv_folder, exist_ok=True)\n",
    "    base_name, csv_path = get_next_csv_name(csv_folder, \"arxiv\")\n",
    "    print(f\"Writing results to CSV: {csv_path}\")\n",
    "\n",
    "    total_found = None\n",
    "    total_downloaded = 0\n",
    "    start = 0\n",
    "\n",
    "    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['arxiv_id', 'title', 'authors', 'published', 'summary', 'categories'])\n",
    "        while total_downloaded < max_total:\n",
    "            feed = arxiv_query(query, max_results=batch_size, start=start)\n",
    "            if total_found is None:\n",
    "                # Get total found from the feed metadata\n",
    "                total_found = int(feed.feed.get('opensearch_totalresults', 0))\n",
    "                print(f\"Total results found: {total_found}\")\n",
    "            entries = feed.entries\n",
    "            if not entries:\n",
    "                break\n",
    "            for entry in entries:\n",
    "                if is_excluded(entry, excluded_terms):\n",
    "                    continue\n",
    "                arxiv_id = entry.id.split('/abs/')[-1]\n",
    "                title = entry.title.replace('\\n', ' ').strip()\n",
    "                authors = '; '.join(author.name for author in entry.authors)\n",
    "                published = entry.published\n",
    "                summary = entry.summary.replace('\\n', ' ').strip()\n",
    "                categories = ', '.join(tag['term'] for tag in entry.tags) if hasattr(entry, 'tags') else ''\n",
    "                writer.writerow([arxiv_id, title, authors, published, summary, categories])\n",
    "                total_downloaded += 1\n",
    "                if total_downloaded >= max_total:\n",
    "                    break\n",
    "            start += batch_size\n",
    "            print(f\"Progress: Downloaded {total_downloaded} / {min(total_found, max_total)}\")\n",
    "            time.sleep(3)  # Respect arXiv API rate limit[2][3]\n",
    "            if len(entries) < batch_size:\n",
    "                break  # No more results\n",
    "    print(f\"Downloaded {total_downloaded} records to {csv_path}\")\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M\")\n",
    "    return total_found, total_downloaded, base_name, timestamp\n",
    "\n",
    "def append_summary_row(summary_folder, base_name, found, downloaded, query, timestamp):\n",
    "    summary_csv_path = os.path.join(summary_folder, \"summary_csv.csv\")\n",
    "    os.makedirs(summary_folder, exist_ok=True)\n",
    "    header = \"source,found,downloaded,query,timestamp\\n\"\n",
    "    row = f\"{base_name},{found},{downloaded},\\\"{query}\\\",{timestamp}\\n\"\n",
    "    file_exists = os.path.exists(summary_csv_path)\n",
    "    is_empty = not file_exists or os.path.getsize(summary_csv_path) == 0\n",
    "    with open(summary_csv_path, 'a', encoding='utf-8', newline='') as f:\n",
    "        if is_empty:\n",
    "            f.write(header)\n",
    "        f.write(row)\n",
    "    print(f\"Summary row added for {base_name}\")\n",
    "\n",
    "# Prepare exclusion terms from your groups dict\n",
    "excluded_terms = [term.strip() for term in groups.get('excluded', '').split('OR') if term.strip()]\n",
    "\n",
    "# Run download and summary\n",
    "found, downloaded, base_name, timestamp = download_arxiv_to_csv_all(\n",
    "    combined_query, csv_folder, excluded_terms, max_total=5000, batch_size=100\n",
    ")\n",
    "append_summary_row(summary_folder, base_name, found, downloaded, combined_query, timestamp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2e70dc",
   "metadata": {},
   "source": [
    "# ArXiv Literature Search Strategy Completed\n",
    "\n",
    "If all scripts have been run successfully (either once or multiple times), you should've received confirmation messages for each block and have at least one csv named arxiv_csv_v(n).csv in your folder defined at the start. Note, that with every single download the code generates an additional version following the naming convention of v1, v2, v3 etc. You should also have a summary table updated with a record of each download you made.\n",
    "\n",
    "Note that your found and download numbers should be different as the exclusion criteria is applied at the download stage for arXiv API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
