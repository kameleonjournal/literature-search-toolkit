{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06b1d3c1",
   "metadata": {},
   "source": [
    "# ArXiv Literature Search Strategy & CSV Generation\n",
    "This notebook allows you to test different keyword strategies and download records using the ArXiv API ()\n",
    "\n",
    "Check the API documentation on: https://info.arxiv.org/help/api/user-manual.html\n",
    "\n",
    "Edit the `groups` and `logic` in the next code cell, then run the subsequent cells to see the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae0369f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'feedparser' is already installed.\n"
     ]
    }
   ],
   "source": [
    "# === SECTION: INSTALL AND IMPORT DEPENDENCIES ===\n",
    "# Commented out: pip install feedparser if not present\n",
    "try:\n",
    "    import feedparser\n",
    "    print(\"'feedparser' is already installed.\")\n",
    "except ModuleNotFoundError:\n",
    "    import subprocess, sys\n",
    "    print(\"'feedparser' not found. Installing now...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"feedparser\"])\n",
    "    import feedparser\n",
    "    print(\"'feedparser' has been installed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dccda83",
   "metadata": {},
   "source": [
    "# 1. Setup & Define Your Folders\n",
    "\n",
    "In the below section uncomment (ctrl+/ on PC or command+/ on Mac) the relevant lines to define the csv and summary folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bd788c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Output folders are set up and ready.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load .env variables\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Get folder paths and API key from environment variables\n",
    "csv_folder = os.getenv(\"CSV_FOLDER\")\n",
    "summary_folder = os.getenv(\"SUMMARY_FOLDER\")\n",
    "\n",
    "# This will create the folders (and parents) if they do NOT exist—does nothing if they do\n",
    "os.makedirs(csv_folder, exist_ok=True)\n",
    "os.makedirs(summary_folder, exist_ok=True)\n",
    "\n",
    "# Check that folders and API key are set\n",
    "missing = []\n",
    "if not os.path.isdir(csv_folder):\n",
    "    missing.append(\"CSV folder\")\n",
    "if not os.path.isdir(summary_folder):\n",
    "    missing.append(\"Summary folder\")\n",
    "\n",
    "if missing:\n",
    "    print(f\"⚠️ WARNING: Please check the following: {', '.join(missing)}\")\n",
    "else:\n",
    "    print(\"✅ Output folders are set up and ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb40ed6b",
   "metadata": {},
   "source": [
    "# 2. Test and adjust your keyword strategy\n",
    "\n",
    "The below 4 sections will help test different keyword groups and their combinations.\n",
    "- 2.1 Run to define groups of keywords using AND/OR rules, then define a combination logic\n",
    "- 2.2. Run to see the number of results returned for each keyword group and the combined query\n",
    "- 2.3. Run to see the first 10 titles for each keyword group\n",
    "- 2.4. Run to see the first 10 titles for the combined keyword group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d22ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range defined: 2016 to 3000\n",
      "Date filter string defined: submittedDate:[201601010000 TO 300001010000]\n",
      "Keyword groups, logic, and date filter defined.\n",
      "Combined arXiv query: (keyword OR keyword) AND (keyword OR keyword) AND (keyword OR keyword) AND (keyword OR keyword) AND submittedDate:[201601010000 TO 300001010000]\n"
     ]
    }
   ],
   "source": [
    "# === SECTION: USER INPUT FOR DATE RANGE ===\n",
    "start_year = 2016  # Example value\n",
    "end_year = 3000    # Example value\n",
    "\n",
    "print(f\"Date range defined: {start_year} to {end_year}\")\n",
    "\n",
    "# === SECTION: DEFINE KEYWORD GROUPS, LOGIC, AND DATE FILTER ===\n",
    "# Exclusion criteria keywords will only applied to combined query at download stage\n",
    "groups = {\n",
    "    'group1': 'keyword OR keyword',\n",
    "    'group2': 'keyword OR keyword',\n",
    "    'group3': 'keyword OR keyword',\n",
    "    'group4': 'keyword OR keyword',\n",
    "    'excluded': 'exclusion OR criteria' \n",
    "}\n",
    "\n",
    "# Date filter for time set onwards\n",
    "date_filter = f'submittedDate:[{start_year}01010000 TO {end_year}01010000]'\n",
    "print(f\"Date filter string defined: {date_filter}\")\n",
    "\n",
    "# Combine logic and add date filter\n",
    "logic = \"({group1}) AND ({group2}) AND ({group3}) AND ({group4}) AND {date_filter}\"\n",
    "combined_query = logic.format(**groups, date_filter=date_filter)\n",
    "print(\"Keyword groups, logic, and date filter defined.\")\n",
    "print(\"Combined arXiv query:\", combined_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4cc04cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "INDIVIDUAL GROUP RESULTS:\n",
      "==================================================\n",
      "\n",
      "GROUP: GROUP1\n",
      "Querying arXiv: http://export.arxiv.org/api/query?search_query=all:neuroimaging%20pipeline%20OR%20MRI%20processing%20OR%20neuroinformatics%20pipeline&start=0&max_results=1\n",
      "Total results for group1: 458989\n",
      "\n",
      "GROUP: GROUP2\n",
      "Querying arXiv: http://export.arxiv.org/api/query?search_query=all:structural%20MRI%20OR%20T1-weighted%20OR%20T2-weighted%20OR%20low-field%20MRI%20OR%20portable%20MRI&start=0&max_results=1\n",
      "Total results for group2: 453094\n",
      "\n",
      "GROUP: GROUP3\n",
      "Querying arXiv: http://export.arxiv.org/api/query?search_query=all:continuous%20integration%20OR%20continuous%20deployment%20OR%20CI/CD%20OR%20containerization%20OR%20containerisation%20OR%20version%20control%20OR%20cloud-based%20OR%20serverless%20OR%20distributed%20storage%20OR%20BIDS%20OR%20flywheel.io%20OR%20github%20OR%20gitlab%20OR%20reproducibility&start=0&max_results=1\n",
      "Total results for group3: 1102006\n",
      "\n",
      "GROUP: GROUP4\n",
      "Querying arXiv: http://export.arxiv.org/api/query?search_query=all:brain%20OR%20neuroimaging&start=0&max_results=1\n",
      "Total results for group4: 17399\n",
      "\n",
      "GROUP: EXCLUDED\n",
      "Querying arXiv: http://export.arxiv.org/api/query?search_query=all:fMRI%20OR%20EEG%20OR%20MEG%20OR%20functional%20connectivity%20OR%20clinical%20trial&start=0&max_results=1\n",
      "Total results for excluded: 670895\n",
      "\n",
      "==================================================\n",
      "COMBINED LOGIC RESULTS:\n",
      "==================================================\n",
      "Querying arXiv: http://export.arxiv.org/api/query?search_query=all:%28neuroimaging%20pipeline%20OR%20MRI%20processing%20OR%20neuroinformatics%20pipeline%29%20AND%20%28structural%20MRI%20OR%20T1-weighted%20OR%20T2-weighted%20OR%20low-field%20MRI%20OR%20portable%20MRI%29%20AND%20%28continuous%20integration%20OR%20continuous%20deployment%20OR%20CI/CD%20OR%20containerization%20OR%20containerisation%20OR%20version%20control%20OR%20cloud-based%20OR%20serverless%20OR%20distributed%20storage%20OR%20BIDS%20OR%20flywheel.io%20OR%20github%20OR%20gitlab%20OR%20reproducibility%29%20AND%20%28brain%20OR%20neuroimaging%29%20AMD%20submittedDate%3A%5B201601010000%20TO%20300001010000%5D&start=0&max_results=1\n",
      "Total results for combined query: 1830\n"
     ]
    }
   ],
   "source": [
    "# === SECTION: RUN ARXIV QUERY AND RETURN TOTAL RESULTS FOR EACH GROUP AND COMBINED QUERY ===\n",
    "import urllib.parse\n",
    "\n",
    "def arxiv_query(query, max_results=1, start=0):\n",
    "    \"\"\"Query arXiv API and return the parsed feed.\"\"\"\n",
    "    import feedparser\n",
    "    base_url = 'http://export.arxiv.org/api/query?'\n",
    "    search_query = urllib.parse.quote(query)\n",
    "    url = f\"{base_url}search_query=all:{search_query}&start={start}&max_results={max_results}\"\n",
    "    print(f\"Querying arXiv: {url}\")\n",
    "    feed = feedparser.parse(url)\n",
    "    return feed\n",
    "\n",
    "# Print total results for each group\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INDIVIDUAL GROUP RESULTS:\")\n",
    "print(\"=\"*50)\n",
    "for name, query in groups.items():\n",
    "    print(f\"\\nGROUP: {name.upper()}\")\n",
    "    feed = arxiv_query(query, max_results=1)  # Only need 1 result to get total count\n",
    "    total_results = feed.feed.get('opensearch_totalresults', 'unknown')\n",
    "    print(f\"Total results for {name}: {total_results}\")\n",
    "\n",
    "# Print total results for combined query\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMBINED LOGIC RESULTS:\")\n",
    "print(\"=\"*50)\n",
    "feed = arxiv_query(combined_query, max_results=1)\n",
    "total_results = feed.feed.get('opensearch_totalresults', 'unknown')\n",
    "print(f\"Total results for combined query: {total_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd04045b",
   "metadata": {},
   "source": [
    "The next block will show the first 10 titles for each keyword group (except the excluded keyword group).\n",
    "\n",
    "Based on this, you can go back and adust your keyword groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e28fd501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FIRST 10 TITLES FOR GROUP: GROUP1\n",
      "==================================================\n",
      "Querying arXiv: http://export.arxiv.org/api/query?search_query=all:neuroimaging%20pipeline%20OR%20MRI%20processing%20OR%20neuroinformatics%20pipeline&start=0&max_results=10\n",
      "1. Machine Learning pipeline for discovering neuroimaging-based biomarkers   in neurology and psychiatry\n",
      "2. An Analysis of Performance Bottlenecks in MRI Pre-Processing\n",
      "3. CompressedMediQ: Hybrid Quantum Machine Learning Pipeline for   High-Dimensional Neuroimaging Data\n",
      "4. Pipeline-Invariant Representation Learning for Neuroimaging\n",
      "5. The Developing Human Connectome Project: A Fast Deep Learning-based   Pipeline for Neonatal Cortical Surface Reconstruction\n",
      "6. JUMP: A joint multimodal registration pipeline for neuroimaging with   minimal preprocessing\n",
      "7. Clinica: an open source software platform for reproducible clinical   neuroscience studies\n",
      "8. FastSurfer -- A fast and accurate deep learning based neuroimaging   pipeline\n",
      "9. Mitigating analytical variability in fMRI results with style transfer\n",
      "10. Minimal Specifications for Non-Human Primate MRI: Challenges in   Standardizing and Harmonizing Data Collection\n",
      "\n",
      "==================================================\n",
      "FIRST 10 TITLES FOR GROUP: GROUP2\n",
      "==================================================\n",
      "Querying arXiv: http://export.arxiv.org/api/query?search_query=all:structural%20MRI%20OR%20T1-weighted%20OR%20T2-weighted%20OR%20low-field%20MRI%20OR%20portable%20MRI&start=0&max_results=10\n",
      "1. MR imaging in the low-field: Leveraging the power of machine learning\n",
      "2. From Low Field to High Value: Robust Cortical Mapping from Low-Field MRI\n",
      "3. Lung Imaging with UTE MRI\n",
      "4. Synthetic Low-Field MRI Super-Resolution Via Nested U-Net Architecture\n",
      "5. Joint CS-MRI Reconstruction and Segmentation with a Unified Deep Network\n",
      "6. Deep Attentive Wasserstein Generative Adversarial Networks for MRI   Reconstruction with Recurrent Context-Awareness\n",
      "7. Portable simulation framework for diffusion MRI\n",
      "8. Unpaired MRI Super Resolution with Contrastive Learning\n",
      "9. 7 Tesla multimodal MRI dataset of ex-vivo human brain\n",
      "10. Accelerated respiratory-resolved 4D-MRI with separable spatio-temporal   neural networks\n",
      "\n",
      "==================================================\n",
      "FIRST 10 TITLES FOR GROUP: GROUP3\n",
      "==================================================\n",
      "Querying arXiv: http://export.arxiv.org/api/query?search_query=all:continuous%20integration%20OR%20continuous%20deployment%20OR%20CI/CD%20OR%20containerization%20OR%20containerisation%20OR%20version%20control%20OR%20cloud-based%20OR%20serverless%20OR%20distributed%20storage%20OR%20BIDS%20OR%20flywheel.io%20OR%20github%20OR%20gitlab%20OR%20reproducibility&start=0&max_results=10\n",
      "1. A First Look at CI/CD Adoptions in Open-Source Android Apps\n",
      "2. CI/CD Configuration Practices in Open-Source Android Apps: An Empirical   Study\n",
      "3. Analyzing the Effects of CI/CD on Open Source Repositories in GitHub and   GitLab\n",
      "4. Ambush from All Sides: Understanding Security Threats in Open-Source   Software CI/CD Pipelines\n",
      "5. Enhancing Software Supply Chain Security Through STRIDE-Based Threat   Modelling of CI/CD Pipelines\n",
      "6. On Continuous Integration / Continuous Delivery for Automated Deployment   of Machine Learning Models using MLOps\n",
      "7. Exploring the Impact of Integrating UI Testing in CI/CD Workflows on   GitHub\n",
      "8. A Systematic Literature Review on Continuous Integration and Deployment   (CI/CD) for Secure Cloud Computing\n",
      "9. Automating the Training and Deployment of Models in MLOps by Integrating   Systems with Machine Learning\n",
      "10. The GitHub Development Workflow Automation Ecosystems\n",
      "\n",
      "==================================================\n",
      "FIRST 10 TITLES FOR GROUP: GROUP4\n",
      "==================================================\n",
      "Querying arXiv: http://export.arxiv.org/api/query?search_query=all:brain%20OR%20neuroimaging&start=0&max_results=10\n",
      "1. Empowering Functional Neuroimaging: A Pre-trained Generative Framework   for Unified Representation of Neural Signals\n",
      "2. CATD: Unified Representation Learning for EEG-to-fMRI Cross-Modal   Generation\n",
      "3. Interpretability of Multivariate Brain Maps in Brain Decoding:   Definition and Quantification\n",
      "4. A Review on Image- and Network-based Brain Data Analysis Techniques for   Alzheimer's Disease Diagnosis Reveals a Gap in Developing Predictive Methods   for Prognosis\n",
      "5. Probabilistic prediction of neurological disorders with a statistical   assessment of neuroimaging data modalities\n",
      "6. Drowsiness detection using combined neuroimaging: Overview and   Challenges\n",
      "7. Diffusion Models for Computational Neuroimaging: A Survey\n",
      "8. A Domain Guided CNN Architecture for Predicting Age from Structural   Brain Images\n",
      "9. ERNet: Unsupervised Collective Extraction and Registration in   Neuroimaging Data\n",
      "10. Multiple Comparison Procedures for Neuroimaging Genomewide Association   Studies\n",
      "\n",
      "==================================================\n",
      "FIRST 10 TITLES FOR GROUP: EXCLUDED\n",
      "==================================================\n",
      "Querying arXiv: http://export.arxiv.org/api/query?search_query=all:fMRI%20OR%20EEG%20OR%20MEG%20OR%20functional%20connectivity%20OR%20clinical%20trial&start=0&max_results=10\n",
      "1. A three domain covariance framework for EEG/MEG data\n",
      "2. Combined MEG and fMRI Exponential Random Graph Modeling for inferring   functional Brain Connectivity\n",
      "3. Towards Neural Foundation Models for Vision: Aligning EEG, MEG, and fMRI   Representations for Decoding, Encoding, and Modality Conversion\n",
      "4. Dynamic EEG-fMRI mapping: Revealing the relationship between brain   connectivity and cognitive state\n",
      "5. Classification of weak multi-view signals by sharing factors in a   mixture of Bayesian group factor analyzers\n",
      "6. Emotion self-regulation training in major depressive disorder using   simultaneous real-time fMRI and EEG neurofeedback\n",
      "7. A comparison of single-trial EEG classification and EEG-informed fMRI   across three MR compatible EEG recording systems\n",
      "8. Effects of simultaneous real-time fMRI and EEG neurofeedback in major   depressive disorder evaluated with brain electromagnetic tomography\n",
      "9. Scaling laws for decoding images from brain activity\n",
      "10. Normative brain mapping using scalp EEG and potential clinical   application\n"
     ]
    }
   ],
   "source": [
    "# === SECTION: PRINT FIRST 10 TITLES FOR EACH GROUP ===\n",
    "for name, query in groups.items():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"FIRST 10 TITLES FOR GROUP: {name.upper()}\")\n",
    "    print(\"=\"*50)\n",
    "    feed = arxiv_query(query, max_results=10)\n",
    "    for i, entry in enumerate(feed.entries, 1):\n",
    "        # Clean the title before using it in the f-string\n",
    "        clean_title = entry.title.strip().replace('\\n', ' ')\n",
    "        print(f\"{i}. {clean_title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14de2937",
   "metadata": {},
   "source": [
    "The next block will show the first 10 titles for the combined query.\n",
    "\n",
    "Based on the results you can go back and adjust your groups and logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5d8febb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FIRST 10 TITLES FOR COMBINED QUERY:\n",
      "==================================================\n",
      "Querying arXiv: http://export.arxiv.org/api/query?search_query=all:%28neuroimaging%20pipeline%20OR%20MRI%20processing%20OR%20neuroinformatics%20pipeline%29%20AND%20%28structural%20MRI%20OR%20T1-weighted%20OR%20T2-weighted%20OR%20low-field%20MRI%20OR%20portable%20MRI%29%20AND%20%28continuous%20integration%20OR%20continuous%20deployment%20OR%20CI/CD%20OR%20containerization%20OR%20containerisation%20OR%20version%20control%20OR%20cloud-based%20OR%20serverless%20OR%20distributed%20storage%20OR%20BIDS%20OR%20flywheel.io%20OR%20github%20OR%20gitlab%20OR%20reproducibility%29%20AND%20%28brain%20OR%20neuroimaging%29%20AMD%20submittedDate%3A%5B201601010000%20TO%20300001010000%5D&start=0&max_results=10\n",
      "1. Clinica: an open source software platform for reproducible clinical   neuroscience studies\n",
      "2. FastSurferVINN: Building Resolution-Independence into Deep Learning   Segmentation Methods -- A Solution for HighRes Brain MRI\n",
      "3. The Developing Human Connectome Project: A Fast Deep Learning-based   Pipeline for Neonatal Cortical Surface Reconstruction\n",
      "4. Minimal Specifications for Non-Human Primate MRI: Challenges in   Standardizing and Harmonizing Data Collection\n",
      "5. Synthesizing PET images from High-field and Ultra-high-field MR images   Using Joint Diffusion Attention Model\n",
      "6. Benchmarking the Reproducibility of Brain MRI Segmentation Across   Scanners and Time\n",
      "7. CompressedMediQ: Hybrid Quantum Machine Learning Pipeline for   High-Dimensional Neuroimaging Data\n",
      "8. Detecting Schizophrenia with 3D Structural Brain MRI Using Deep Learning\n",
      "9. Residual and Plain Convolutional Neural Networks for 3D Brain MRI   Classification\n",
      "10. An Explainable Diagnostic Framework for Neurodegenerative Dementias via   Reinforcement-Optimized LLM Reasoning\n"
     ]
    }
   ],
   "source": [
    "# === SECTION: PRINT FIRST 10 TITLES FOR COMBINED QUERY ===\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FIRST 10 TITLES FOR COMBINED QUERY:\")\n",
    "print(\"=\"*50)\n",
    "feed = arxiv_query(combined_query, max_results=10)\n",
    "for i, entry in enumerate(feed.entries, 1):\n",
    "    clean_title = entry.title.strip().replace('\\n', ' ')\n",
    "    print(f\"{i}. {clean_title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011b84a3",
   "metadata": {},
   "source": [
    "# 2. Export ArXiv Results to CSV\n",
    "\n",
    "The below script will use your combined query to download titles and abstracts and save them to a CSV file, including author name, title, abstract, year and doi. It will also update the summary table to include the total of found and downloaded records, the source the final query and a timestamp for record keeping purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa622120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results to CSV: C:/Users/petra/Documents/UniKCL/Workshop/csvs/arxiv_v1.csv\n",
      "Querying arXiv: http://export.arxiv.org/api/query?search_query=all:%28neuroimaging%20pipeline%20OR%20MRI%20processing%20OR%20neuroinformatics%20pipeline%29%20AND%20%28structural%20MRI%20OR%20T1-weighted%20OR%20T2-weighted%20OR%20low-field%20MRI%20OR%20portable%20MRI%29%20AND%20%28continuous%20integration%20OR%20continuous%20deployment%20OR%20CI/CD%20OR%20containerization%20OR%20containerisation%20OR%20version%20control%20OR%20cloud-based%20OR%20serverless%20OR%20distributed%20storage%20OR%20BIDS%20OR%20flywheel.io%20OR%20github%20OR%20gitlab%20OR%20reproducibility%29%20AND%20%28brain%20OR%20neuroimaging%29%20AMD%20submittedDate%3A%5B201601010000%20TO%20300001010000%5D&start=0&max_results=100\n",
      "Total results found: 1830\n",
      "Progress: Downloaded 87 / 1830\n",
      "Querying arXiv: http://export.arxiv.org/api/query?search_query=all:%28neuroimaging%20pipeline%20OR%20MRI%20processing%20OR%20neuroinformatics%20pipeline%29%20AND%20%28structural%20MRI%20OR%20T1-weighted%20OR%20T2-weighted%20OR%20low-field%20MRI%20OR%20portable%20MRI%29%20AND%20%28continuous%20integration%20OR%20continuous%20deployment%20OR%20CI/CD%20OR%20containerization%20OR%20containerisation%20OR%20version%20control%20OR%20cloud-based%20OR%20serverless%20OR%20distributed%20storage%20OR%20BIDS%20OR%20flywheel.io%20OR%20github%20OR%20gitlab%20OR%20reproducibility%29%20AND%20%28brain%20OR%20neuroimaging%29%20AMD%20submittedDate%3A%5B201601010000%20TO%20300001010000%5D&start=100&max_results=100\n",
      "Downloaded 87 records to C:/Users/petra/Documents/UniKCL/Workshop/csvs/arxiv_v1.csv\n",
      "Summary row added for arxiv_v1\n"
     ]
    }
   ],
   "source": [
    "# === SECTION: EXPORT ARXIV RESULTS TO CSV (ALL RESULTS, WITH EXCLUSION) ===\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def get_next_csv_name(folder, base_name):\n",
    "    i = 1\n",
    "    while True:\n",
    "        csv_name = f\"{base_name}_v{i}.csv\"\n",
    "        csv_path = os.path.join(folder, csv_name)\n",
    "        if not os.path.exists(csv_path):\n",
    "            return f\"{base_name}_v{i}\", csv_path\n",
    "        i += 1\n",
    "\n",
    "def is_excluded(entry, excluded_terms):\n",
    "    \"\"\"Return True if any excluded term is found in the title or summary.\"\"\"\n",
    "    text = (entry.title + \" \" + entry.summary).lower()\n",
    "    return any(term.lower() in text for term in excluded_terms)\n",
    "\n",
    "def download_arxiv_to_csv_all(query, csv_folder, excluded_terms, max_total=5000, batch_size=100):\n",
    "    import feedparser\n",
    "    os.makedirs(csv_folder, exist_ok=True)\n",
    "    base_name, csv_path = get_next_csv_name(csv_folder, \"arxiv\")\n",
    "    print(f\"Writing results to CSV: {csv_path}\")\n",
    "\n",
    "    total_found = None\n",
    "    total_downloaded = 0\n",
    "    start = 0\n",
    "\n",
    "    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['arxiv_id', 'title', 'authors', 'published', 'summary', 'categories'])\n",
    "        while total_downloaded < max_total:\n",
    "            feed = arxiv_query(query, max_results=batch_size, start=start)\n",
    "            if total_found is None:\n",
    "                # Get total found from the feed metadata\n",
    "                total_found = int(feed.feed.get('opensearch_totalresults', 0))\n",
    "                print(f\"Total results found: {total_found}\")\n",
    "            entries = feed.entries\n",
    "            if not entries:\n",
    "                break\n",
    "            for entry in entries:\n",
    "                if is_excluded(entry, excluded_terms):\n",
    "                    continue\n",
    "                arxiv_id = entry.id.split('/abs/')[-1]\n",
    "                title = entry.title.replace('\\n', ' ').strip()\n",
    "                authors = '; '.join(author.name for author in entry.authors)\n",
    "                published = entry.published\n",
    "                summary = entry.summary.replace('\\n', ' ').strip()\n",
    "                categories = ', '.join(tag['term'] for tag in entry.tags) if hasattr(entry, 'tags') else ''\n",
    "                writer.writerow([arxiv_id, title, authors, published, summary, categories])\n",
    "                total_downloaded += 1\n",
    "                if total_downloaded >= max_total:\n",
    "                    break\n",
    "            start += batch_size\n",
    "            print(f\"Progress: Downloaded {total_downloaded} / {min(total_found, max_total)}\")\n",
    "            time.sleep(3)  # Respect arXiv API rate limit[2][3]\n",
    "            if len(entries) < batch_size:\n",
    "                break  # No more results\n",
    "    print(f\"Downloaded {total_downloaded} records to {csv_path}\")\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M\")\n",
    "    return total_found, total_downloaded, base_name, timestamp\n",
    "\n",
    "def append_summary_row(summary_folder, base_name, found, downloaded, query, timestamp):\n",
    "    summary_csv_path = os.path.join(summary_folder, \"summary_csv.csv\")\n",
    "    os.makedirs(summary_folder, exist_ok=True)\n",
    "    header = \"source,found,downloaded,query,timestamp\\n\"\n",
    "    row = f\"{base_name},{found},{downloaded},\\\"{query}\\\",{timestamp}\\n\"\n",
    "    file_exists = os.path.exists(summary_csv_path)\n",
    "    is_empty = not file_exists or os.path.getsize(summary_csv_path) == 0\n",
    "    with open(summary_csv_path, 'a', encoding='utf-8', newline='') as f:\n",
    "        if is_empty:\n",
    "            f.write(header)\n",
    "        f.write(row)\n",
    "    print(f\"Summary row added for {base_name}\")\n",
    "\n",
    "# Prepare exclusion terms from your groups dict\n",
    "excluded_terms = [term.strip() for term in groups.get('excluded', '').split('OR') if term.strip()]\n",
    "\n",
    "# Run download and summary\n",
    "found, downloaded, base_name, timestamp = download_arxiv_to_csv_all(\n",
    "    combined_query, csv_folder, excluded_terms, max_total=5000, batch_size=100\n",
    ")\n",
    "append_summary_row(summary_folder, base_name, found, downloaded, combined_query, timestamp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2e70dc",
   "metadata": {},
   "source": [
    "# ArXiv Literature Search Strategy Completed\n",
    "\n",
    "If all scripts have been run successfully (either once or multiple times), you should've received confirmation messages for each block and have at least one csv named arxiv_csv_v(n).csv in your folder defined at the start. Note, that with every single download the code generates an additional version following the naming convention of v1, v2, v3 etc. You should also have a summary table updated with a record of each download you made.\n",
    "\n",
    "Note that your found and download numbers should be different as the exclusion criteria is applied at the download stage for arXiv API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
