{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb676af9",
   "metadata": {},
   "source": [
    "Combine CSVs and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3732bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION: Install missing dependencies\n",
    "print(\"Checking for and installing missing dependencies...\")\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install pandas python-dotenv\n",
    "\n",
    "print(\"Done installing dependencies. Please re-run the notebook from the top if needed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4663b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION: Import libraries\n",
    "print(\"Importing libraries...\")\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import re\n",
    "\n",
    "# SECTION: Load environment variables\n",
    "print(\"Loading environment variables from .env...\")\n",
    "load_dotenv()\n",
    "CSV_FOLDER = os.getenv(\"CSV_FOLDER\")\n",
    "SUMMARY_FOLDER = os.getenv(\"SUMMARY_FOLDER\")\n",
    "\n",
    "if not CSV_FOLDER or not SUMMARY_FOLDER:\n",
    "    raise ValueError(\"Could not find CSV_FOLDER or SUMMARY_FOLDER in .env.\")\n",
    "\n",
    "print(f\"CSV_FOLDER: {CSV_FOLDER}\")\n",
    "print(f\"SUMMARY_FOLDER: {SUMMARY_FOLDER}\")\n",
    "\n",
    "# SECTION: Define canonical column mapping (all known variants)\n",
    "column_map = {\n",
    "    'first_author':   ['first_author', 'first author', 'author'],\n",
    "    'all_authors':    ['all_authors', 'all authors', 'authors'],\n",
    "    'title':          ['title', 'paper title'],\n",
    "    'abstract':       ['abstract', 'summary'],\n",
    "    'year':           ['year', 'publication year', 'pub_year', 'date'],\n",
    "    'doi':            ['doi', 'arxiv id', 'arxiv_id'],\n",
    "}\n",
    "\n",
    "required_final_cols = [\n",
    "    'first_author', 'all_authors', 'title', 'abstract',\n",
    "    'year', 'doi', 'source', 'record type', 'dedup_key'\n",
    "]\n",
    "\n",
    "# flatten aliases for fast lookup\n",
    "flat_map = {}\n",
    "for canon, aliases in column_map.items():\n",
    "    for a in aliases:\n",
    "        flat_map[a.lower()] = canon\n",
    "\n",
    "# SECTION: Identify latest-version CSVs only\n",
    "print(\"\\nFinding all CSV files in the folder...\")\n",
    "csv_files = sorted(glob(os.path.join(CSV_FOLDER, \"*.csv\")))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"No CSV files found in {CSV_FOLDER}\")\n",
    "\n",
    "print(\"All CSV files found:\")\n",
    "for fname in csv_files:\n",
    "    print(\" \", os.path.basename(fname))\n",
    "\n",
    "def extract_base_and_version(filename):\n",
    "    m = re.match(r'(.+?)_v(\\d+)\\.csv$', filename)\n",
    "    if m:\n",
    "        base = m.group(1)\n",
    "        ver = int(m.group(2))\n",
    "        return base, ver\n",
    "    else:\n",
    "        base = filename[:-4] if filename.lower().endswith('.csv') else filename\n",
    "        return base, 0\n",
    "\n",
    "versioned_files = {}\n",
    "for fpath in csv_files:\n",
    "    fname = os.path.basename(fpath)\n",
    "    base, ver = extract_base_and_version(fname)\n",
    "    if base not in versioned_files or ver > versioned_files[base][1]:\n",
    "        versioned_files[base] = (fpath, ver)\n",
    "selected_files = [tpl[0] for tpl in versioned_files.values()]\n",
    "\n",
    "print(\"\\nFiles selected for combining (latest version of each base name):\")\n",
    "for fname in selected_files:\n",
    "    print(\" \", os.path.basename(fname))\n",
    "\n",
    "# User confirmation\n",
    "proceed = input(\"\\nProceed with these files? Type 'y' and Enter to continue, or anything else to stop: \")\n",
    "if proceed.strip().lower() != 'y':\n",
    "    print(\"Aborting as per user request.\")\n",
    "    raise SystemExit()\n",
    "\n",
    "# SECTION: Read, align, and annotate CSVs\n",
    "print(\"\\nReading, aligning, and annotating each CSV file...\")\n",
    "file_records = []\n",
    "for csv_file in selected_files:\n",
    "    try:\n",
    "        df_orig = pd.read_csv(csv_file, dtype=str)\n",
    "        source_name = os.path.basename(csv_file)\n",
    "        # Map columns\n",
    "        newcols = {}\n",
    "        for c in df_orig.columns:\n",
    "            c_key = c.lower().strip()\n",
    "            if c_key in flat_map:\n",
    "                newcols[c] = flat_map[c_key]\n",
    "        df = df_orig.rename(columns=newcols)\n",
    "\n",
    "        # Add missing columns as blank\n",
    "        for canoncol in column_map.keys():\n",
    "            if canoncol not in df.columns:\n",
    "                df[canoncol] = ''\n",
    "\n",
    "        # dedup: use doi if any not-empty, otherwise title\n",
    "        has_doi = ('doi' in df.columns) and df['doi'].str.strip().any()\n",
    "        has_title = ('title' in df.columns) and df['title'].str.strip().any()\n",
    "        if has_doi:\n",
    "            dedup_key = 'doi'\n",
    "            df['dedup_id'] = df['doi']\n",
    "        elif has_title:\n",
    "            dedup_key = 'title'\n",
    "            df['dedup_id'] = df['title']\n",
    "        else:\n",
    "            raise ValueError(f\"File {source_name} missing both usable 'doi' and 'title' columns!\")\n",
    "        df['file_tag'] = source_name\n",
    "        df['dedup_key'] = dedup_key\n",
    "        file_records.append(df)\n",
    "        print(f\"  Read {source_name} ({len(df)} rows, dedupe by '{dedup_key}')\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error reading {csv_file}: {e}\")\n",
    "\n",
    "if not file_records:\n",
    "    raise Exception(\"No valid CSVs read. Aborting.\")\n",
    "\n",
    "# SECTION: Combine, aggregate and deduplicate\n",
    "print(\"\\nCombining records and annotating duplicates...\")\n",
    "combined_df = pd.concat(file_records, ignore_index=True)\n",
    "\n",
    "# Group: list of source files for each dedup value\n",
    "dedup_files = (\n",
    "    combined_df\n",
    "    .groupby('dedup_id')['file_tag']\n",
    "    .agg(lambda files: \";\".join(sorted(set(files))))\n",
    "    .reset_index()\n",
    "    .rename(columns={'file_tag': 'source'})\n",
    ")\n",
    "\n",
    "# count appearances\n",
    "dedup_counts = (\n",
    "    combined_df\n",
    "    .groupby('dedup_id')\n",
    "    .size()\n",
    "    .reset_index(name='dedup_count')\n",
    ")\n",
    "\n",
    "# merge back\n",
    "combined_annotated = pd.merge(combined_df, dedup_files, on='dedup_id', how='left')\n",
    "combined_annotated = pd.merge(combined_annotated, dedup_counts, on='dedup_id', how='left')\n",
    "\n",
    "combined_annotated['record type'] = combined_annotated['dedup_count'].apply(lambda c: \"unique\" if c == 1 else \"duplicate\")\n",
    "\n",
    "# Drop duplicated dedup_id, keeping first\n",
    "final_df = (\n",
    "    combined_annotated\n",
    "    .drop_duplicates(subset=['dedup_id'])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Arrange columns (ensure all present)\n",
    "for col in required_final_cols:\n",
    "    if col not in final_df.columns:\n",
    "        final_df[col] = ''\n",
    "\n",
    "final_df = final_df[required_final_cols]\n",
    "\n",
    "print(f\"Final combined CSV has {len(final_df)} unique deduplicated records.\")\n",
    "\n",
    "# SECTION: Save combined CSV\n",
    "os.makedirs(SUMMARY_FOLDER, exist_ok=True)\n",
    "output_path = os.path.join(SUMMARY_FOLDER, \"combined_csv.csv\")\n",
    "final_df.to_csv(output_path, index=False)\n",
    "print(f\"Combined CSV written to: {output_path}\")\n",
    "\n",
    "# (Optional) Display the first few rows for inspection\n",
    "final_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
