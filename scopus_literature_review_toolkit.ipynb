{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b92b5f04",
   "metadata": {},
   "source": [
    "# Scopus Literature Search Strategy & CSV Generation\n",
    "This notebook allows you to test different keyword strategies for the Scopus API.\n",
    "\n",
    "Obtain your API key through: https://dev.elsevier.com/\n",
    "\n",
    "Edit the `groups` and `logic` in the next code cell, then run the subsequent cells to see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "825c1672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imported all necessary libraries.\n"
     ]
    }
   ],
   "source": [
    "# === SECTION: Import Important Functions ===\n",
    "import os\n",
    "import csv\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "print(\"✅ Imported all necessary libraries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6be0869",
   "metadata": {},
   "source": [
    "# 1. Setup folders and API key\n",
    "\n",
    "In the below section uncomment (ctrl+/ on PC or command+/ on Mac) the relevant lines to define the csv and summary folder and to include your API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd51d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Output folders and API key are set up and ready.\n"
     ]
    }
   ],
   "source": [
    "# === SECTION: USER SETUP (PC/Windows) ===\n",
    "# Comment/uncomment below and edit these variables to match your Windows setup\n",
    "# csv_folder = r\"C:\\Users\\YOUR_USERNAME\\Documents\\csvs\\scopus_csv\"\n",
    "# summary_folder = r\"C:\\Users\\YOUR_USERNAME\\Documents\\csvs\\summaries\"\n",
    "# api_key = \"YOUR_SCOPUS_API_KEY\"  # Replace with your Scopus API key\n",
    "\n",
    "# === SECTION: USER SETUP (Mac) ===\n",
    "# Comment/uncomment below and edit these lines to match your Mac setup\n",
    "csv_folder = r\"/Users/YOUR_USERNAME/Documents/SP/csvs/scopus_csv\"\n",
    "summary_folder = r\"/Users/YOUR_USERNAME/Documents/SP/csvs/summaries\"\n",
    "api_key = \"YOUR_SCOPUS_API_KEY\"  # Replace with your Scopus API key (https://dev.elsevier.com/)\n",
    "\n",
    "# === SECTION: FOLDER CREATION AND CHECK ===\n",
    "import os\n",
    "\n",
    "os.makedirs(csv_folder, exist_ok=True)\n",
    "os.makedirs(summary_folder, exist_ok=True)\n",
    "missing = []\n",
    "if not api_key or api_key == \"YOUR_SCOPUS_API_KEY\":\n",
    "    missing.append(\"API key\")\n",
    "if not os.path.isdir(csv_folder):\n",
    "    missing.append(\"CSV folder\")\n",
    "if not os.path.isdir(summary_folder):\n",
    "    missing.append(\"Summary folder\")\n",
    "\n",
    "if missing:\n",
    "    print(f\"⚠️ WARNING: Please check the following: {', '.join(missing)}\")\n",
    "else:\n",
    "    print(\"✅ Output folders and API key are set up and ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03c0f97",
   "metadata": {},
   "source": [
    "# 2. Test and adjust your keyword strategy\n",
    "\n",
    "The below 4 sections will help test different keyword groups and their combinations.\n",
    "- 2.1 Run to define groups of keywords and your exclusion keyword group using AND/OR rules, then define a combination logic\n",
    "- 2.2. Run to see the number of results returned for each keyword group and the combined query\n",
    "- 2.3. Run to see the first 10 titles for each keyword group\n",
    "- 2.4. Run to see the first 10 titles for the combined keyword group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21b6a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Keyword groups and logic defined.\n",
      "Year filter: 2016-2025\n",
      "Combined Scopus query: (TITLE-ABS-KEY(\"neuroimaging pipeline\" OR \"MRI processing\" OR \"neuroinformatics pipeline\")) AND (TITLE-ABS-KEY(\"structural MRI\" OR \"T1-weighted\" OR \"T2-weighted\" OR \"low-field MRI\" OR \"portable MRI\")) AND (TITLE-ABS-KEY(\"continuous integration\" OR \"continuous deployment\" OR \"CI/CD\" OR containerization OR containerisation OR \"version control\" OR \"cloud-based\" OR serverless OR \"distributed storage\" OR BIDS OR \"flywheel.io\" OR github OR gitlab OR reproducibility)) AND (TITLE-ABS-KEY(brain OR neuroimaging)) AND NOT TITLE-ABS-KEY(fMRI OR EEG OR MEG OR \"functional connectivity\" OR \"clinical trial\")\n"
     ]
    }
   ],
   "source": [
    "# === SECTION: Define Groups, Logic, and Year Filter (CORRECTED) ===\n",
    "from datetime import datetime\n",
    "\n",
    "groups = {\n",
    "    'group1': 'TITLE-ABS-KEY(keyword OR \"keyword\")',\n",
    "    'group2': 'TITLE-ABS-KEY(\"keyword\" OR \"keyword\")',\n",
    "    'group3': 'TITLE-ABS-KEY(\"keyword\" OR \"keyword\")',\n",
    "    'group4': 'TITLE-ABS-KEY(\"keyword\" OR \"keyword\")',\n",
    "    'excluded': 'AND NOT TITLE-ABS-KEY(keyword OR \"keyword\")'\n",
    "}\n",
    "\n",
    "logic = \"({group1}) AND ({group2}) AND ({group3}) AND ({group4}) {excluded}\"\n",
    "combined_query = logic.format(**groups)\n",
    "\n",
    "year_from = 2016\n",
    "year_to = datetime.now().year\n",
    "\n",
    "print(f\"✅ Keyword groups and logic defined.\\nYear filter: {year_from}-{year_to}\")\n",
    "print(\"Combined Scopus query:\", combined_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "921318a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year filter applied to all queries: 2016-2025\n",
      "\n",
      "==================================================\n",
      "INDIVIDUAL GROUP RESULTS:\n",
      "==================================================\n",
      "GROUP1                   : 153 results\n",
      "GROUP2                   : 56537 results\n",
      "GROUP3                   : 398942 results\n",
      "GROUP4                   : 1119364 results\n",
      "\n",
      "==================================================\n",
      "COMBINED LOGIC RESULTS:\n",
      "==================================================\n",
      "Logic: ({group1}) AND ({group2}) AND ({group3}) AND ({group4}) {excluded}\n",
      "\n",
      "Combined query: (TITLE-ABS-KEY(\"neuroimaging pipeline\" OR \"MRI processing\" OR \"neuroinformatics pipeline\")) AND (TITLE-ABS-KEY(\"structural MRI\" OR \"T1-weighted\" OR \"T2-weighted\" OR \"low-field MRI\" OR \"portable MRI\")) AND (TITLE-ABS-KEY(\"continuous integration\" OR \"continuous deployment\" OR \"CI/CD\" OR containerization OR containerisation OR \"version control\" OR \"cloud-based\" OR serverless OR \"distributed storage\" OR BIDS OR \"flywheel.io\" OR github OR gitlab OR reproducibility)) AND (TITLE-ABS-KEY(brain OR neuroimaging)) AND NOT TITLE-ABS-KEY(fMRI OR EEG OR MEG OR \"functional connectivity\" OR \"clinical trial\")\n",
      "Combined results: 7\n"
     ]
    }
   ],
   "source": [
    "# === SECTION: Run API Query and Return Total Results for Each Group and Combined Query ===\n",
    "\n",
    "def run_scopus_query(query, api_key, year_from=None, year_to=None, max_records=1):\n",
    "    base_url = \"https://api.elsevier.com/content/search/scopus\"\n",
    "    \n",
    "    # Build date filter if years are specified\n",
    "    date_filter = \"\"\n",
    "    if year_from and year_to:\n",
    "        date_filter = f\" AND PUBYEAR > {year_from-1} AND PUBYEAR < {year_to+1}\"\n",
    "    elif year_from:\n",
    "        date_filter = f\" AND PUBYEAR > {year_from-1}\"\n",
    "    elif year_to:\n",
    "        date_filter = f\" AND PUBYEAR < {year_to+1}\"\n",
    "    \n",
    "    full_query = query + date_filter\n",
    "    \n",
    "    params = {\n",
    "        'query': full_query,\n",
    "        'count': min(max_records, 25),  # Scopus API limit is 25 per request\n",
    "        'start': 0,\n",
    "        'view': 'STANDARD'\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        'Accept': 'application/json',\n",
    "        'X-ELS-APIKey': api_key\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, params=params, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        total = int(data.get('search-results', {}).get('opensearch:totalResults', 0))\n",
    "        articles = data.get('search-results', {}).get('entry', [])\n",
    "        \n",
    "        return total, articles\n",
    "        \n",
    "    except:\n",
    "        return 0, []\n",
    "\n",
    "print(\"Year filter applied to all queries:\", f\"{year_from}-{year_to}\\n\")\n",
    "print(\"=\"*50)\n",
    "print(\"INDIVIDUAL GROUP RESULTS:\")\n",
    "print(\"=\"*50)\n",
    "group_results = {}\n",
    "for name, query in groups.items():\n",
    "    if name != 'excluded':\n",
    "        count, _ = run_scopus_query(query, api_key, year_from, year_to)\n",
    "        group_results[name] = count\n",
    "        print(f\"{name.upper():<25}: {count} results\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMBINED LOGIC RESULTS:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Logic: {logic}\\n\")\n",
    "print(f\"Combined query: {combined_query}\")\n",
    "combined_count, combined_articles = run_scopus_query(combined_query, api_key, year_from, year_to)\n",
    "print(f\"Combined results: {combined_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992b1d62",
   "metadata": {},
   "source": [
    "The next block will show the first 10 titles for each keyword group (except the excluded keyword group).\n",
    "\n",
    "Based on this, you can go back and adust your keyword groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b70b8676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "GROUP1 (First 10 Titles):\n",
      "==============================\n",
      "1. Fibre density and cross-section associate with hallmark pathology in early Alzheimer’s disease\n",
      "2. A novel approach for the detection of brain tumor and its classification via independent component analysis\n",
      "3. An intensive non-invasive protocol combining non-surgical spinal decompression and supportive physiotherapeutic modalities in the treatment of double-level disc herniation at L4-L5 and L5-S1: A case report\n",
      "4. Dynamic Contrast-enhanced MRI Processing Comparison for Distinguishing True Progression from Pseudoprogression in High-grade Glioma\n",
      "5. Enhancing Prenatal Diagnosis: Automated Fetal Brain MRI Morphometry\n",
      "6. Prognostic Survival Analysis for AD Diagnosis and Progression Using MRI Data: An AI-Based Approach\n",
      "7. A novel approach for the detection of brain tumor and its classification via end-to-end vision transformer - CNN architecture\n",
      "8. Advancing Thalamic Nuclei Segmentation: The Impact of Compressed Sensing on MRI Processing\n",
      "9. Empirical Perspectives on Machine Learning Models for Contextual Analysis of MRI Scans\n",
      "10. Magnetic resonance imaging studies on acupuncture therapy in cerebral ischemic stroke: A systematic review\n",
      "\n",
      "==============================\n",
      "GROUP2 (First 10 Titles):\n",
      "==============================\n",
      "1. Small Magnets, Big Future: Low-Field MRI Technology and Clinical Utility\n",
      "2. A comprehensive dataset of germinoma on MRI/CT with clinical and radiomic data\n",
      "3. Comparative evaluation of non-contrast MRI versus gadoxetic acid-enhanced abbreviated protocols in detecting colorectal liver metastases\n",
      "4. Deep learning for quality assessment of axial T2-weighted prostate MRI: a tool to reduce unnecessary rescanning\n",
      "5. Ex vivo study of Porcine esophagus magnetic resonance imaging with a compact endoluminal coil\n",
      "6. Glomus tumor of the patellar tendon: a case report and review of the literature\n",
      "7. Feasibility of virtual T2-weighted fat-saturated breast MRI images by convolutional neural networks\n",
      "8. An Annotated Multi-Site and Multi-Contrast Magnetic Resonance Imaging Dataset for the study of the Human Tongue Musculature\n",
      "9. Quantification of liver fat fraction using T1-weighted mDixon MRI in young patients with ataxia telangiectasia undergoing whole-body MRI: an exploratory study\n",
      "10. Surgical management of a giant glial hamartoma in a pediatric patient: a case report\n",
      "\n",
      "==============================\n",
      "GROUP3 (First 10 Titles):\n",
      "==============================\n",
      "1. Use of artificial intelligence to transcribe and summarise general practice consultations\n",
      "2. Defect dipole gradient design in (K,Na)NbO3-based piezoelectric ceramics enabling controllable ultrahigh bending deformation\n",
      "3. Metric Properties of the Test of Infant Motor Performance in Colombian Children\n",
      "4. A cloud-based intelligent reconstruction method for low-sampling-rate signals in remote condition monitoring of hydraulic pumps\n",
      "5. MFFNet: Joint demoiréing and super-resolution\n",
      "6. Dynamic range compression method for high radiometric resolution remote sensing images using contrastive learning\n",
      "7. Development of a sustainable fluorescence-based approach using erythrosine B dye for nanolevel quantification of bepotastine in eye drops and aqueous humor\n",
      "8. PRNN: Pareto-Guided recursive neural network embedded approach for biomarker discovery in breast cancer multi-omics dataset\n",
      "9. Rapid visual authentication of high-temperature Daqu Baijiu using porphyrin signal amplification and smartphone-based cloud machine learning\n",
      "10. Deep learning in flower quantification of Catharanthus roseus (L.) G. Don\n",
      "\n",
      "==============================\n",
      "GROUP4 (First 10 Titles):\n",
      "==============================\n",
      "1. Neuroprotective Effects of Early TLR4 Blockade with Compound C34 in Temporal Lobe Epilepsy: Alleviation of Neuroinflammation and Apoptosis\n",
      "2. Detection and quantification of ergothioneine in human serum using surface enhanced Raman scattering (SERS)†\n",
      "3. Use of artificial intelligence for detection and managing atrial fibrillation—narrative review of the current literature\n",
      "4. Gadd45b alleviates white matter damage in chronic ischemic rats by modulating astrocyte phenotype\n",
      "5. Action mechanism of intrathecal transplantation of human umbilical cord mesenchymal stem cell-derived exosomes for repair of spinal cord injury under neuroendoscopy\n",
      "6. Mechanism of erythropoietin/erythropoietin receptor signaling pathway in regulating osteogenic differentiation of periodontal ligament stem cells\n",
      "7. Chinese herbal prescription combined with head acupuncture exercise therapy improves limb spasticity in rats with ischemic stroke\n",
      "8. EDANet: Efficient domain-adaptive attention neural network for EEG classification of motor imagery\n",
      "9. Functional connectivity guided deep neural network for decoding high-level visual imagery\n",
      "10. Exploring the Impact of Citalopram on Human Colon Cells: Insights into Antidepressant Action Beyond the Brain\n"
     ]
    }
   ],
   "source": [
    "# === SECTION: Show First 10 Titles for Each Keyword Group (Except NOT Group) ===\n",
    "\n",
    "for name, query in groups.items():\n",
    "    if name == 'excluded':\n",
    "        continue\n",
    "    print(f\"\\n{'='*30}\\n{name.upper()} (First 10 Titles):\\n{'='*30}\")\n",
    "    _, articles = run_scopus_query(query, api_key, year_from, year_to, max_records=10)\n",
    "    for i, article in enumerate(articles, 1):\n",
    "        title = article.get('dc:title', 'No Title Available')\n",
    "        print(f\"{i}. {title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65144f4",
   "metadata": {},
   "source": [
    "The next block will show the first 10 titles for the combined query.\n",
    "\n",
    "Based on the results you can go back and adjust your groups and logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "367c6d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "COMBINED QUERY (First 10 Titles):\n",
      "==============================\n",
      "1. Reproducibility evaluation of the effects of MRI defacing on brain segmentation\n",
      "2. Reproducibility and Reliability of Computing Models in Segmentation and Volumetric Measurement of Brain\n",
      "3. An automatic and accurate deep learning-based neuroimaging pipeline for the neonatal brain\n",
      "4. PhiPipe: A multi-modal MRI data processing pipeline with test–retest reliability and predicative validity assessments\n",
      "5. FastSurfer - A fast and accurate deep learning based neuroimaging pipeline\n",
      "6. Test-retest reliability and sample size estimates after MRI scanner relocation\n",
      "7. A short-term scan-rescan reliability test measuring brain tissue and subcortical hyperintensity volumetrics obtained using the lesion explorer structural MRI processing pipeline\n"
     ]
    }
   ],
   "source": [
    "# === SECTION: Show First 10 Titles for Combined Keyword Group ===\n",
    "\n",
    "print(f\"\\n{'='*30}\\nCOMBINED QUERY (First 10 Titles):\\n{'='*30}\")\n",
    "_, articles = run_scopus_query(combined_query, api_key, year_from, year_to, max_records=10)\n",
    "for i, article in enumerate(articles, 1):\n",
    "    title = article.get('dc:title', 'No Title Available')\n",
    "    print(f\"{i}. {title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15299e98",
   "metadata": {},
   "source": [
    "# 2. Export Scopus Results to CSV\n",
    "\n",
    "The below script will use your combined query to download titles and abstracts and save them to a CSV file, including author name, title, abstract, year and doi. It will also update the summary table to include the total of found and downloaded records, the source the final query and a timestamp for record keeping purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a22f5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Starting final export with combined query...\n",
      "🚀 Starting Scopus data export...\n",
      "🔄 Starting data collection for query...\n",
      "📊 Total results available: 7\n",
      "📥 Collected 7 articles so far...\n",
      "🔚 Reached end of available results\n",
      "💾 Writing 7 articles to scopus_csv_v3.csv\n",
      "✅ Exported 7 records to /Users/petrakisherczegh/Documents/SP/csvs/scopus_csv/scopus_csv_v3.csv\n",
      "✅ Summary row updated.\n"
     ]
    }
   ],
   "source": [
    "# === SECTION: Download CSV and Update Summary ===\n",
    "\n",
    "def extract_first_author(authors_field):\n",
    "    \"\"\"\n",
    "    Extract first author from Scopus author field - CORRECTED VERSION\n",
    "    \"\"\"\n",
    "    if isinstance(authors_field, list) and authors_field:\n",
    "        first = authors_field[0]  # Get first author from list\n",
    "        if isinstance(first, dict):\n",
    "            return first.get('authname', '') or first.get('ce:indexed-name', '')\n",
    "    elif isinstance(authors_field, dict):\n",
    "        # Single author case\n",
    "        return authors_field.get('authname', '') or authors_field.get('ce:indexed-name', '')\n",
    "    elif isinstance(authors_field, str):\n",
    "        return authors_field\n",
    "    return ''\n",
    "\n",
    "def get_next_versioned_filename(folder, base_name=\"scopus_csv\", ext=\".csv\"):\n",
    "    \"\"\"\n",
    "    Generate next available versioned filename\n",
    "    \"\"\"\n",
    "    i = 1\n",
    "    while True:\n",
    "        filename = f\"{base_name}_v{i}{ext}\"\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            return filename, filepath, i\n",
    "        i += 1\n",
    "\n",
    "def ensure_newline_at_end(filepath):\n",
    "    \"\"\"Ensures the file ends with a newline before appending.\"\"\"\n",
    "    if os.path.isfile(filepath):\n",
    "        with open(filepath, 'rb+') as f:\n",
    "            f.seek(-1, os.SEEK_END)\n",
    "            last_char = f.read(1)\n",
    "            if last_char != b'\\n':\n",
    "                f.write(b'\\n')\n",
    "\n",
    "def get_all_scopus_results(query, api_key, year_from, year_to, max_records=1000):\n",
    "    \"\"\"\n",
    "    Retrieve all results from Scopus API with pagination\n",
    "    \"\"\"\n",
    "    base_url = \"https://api.elsevier.com/content/search/scopus\"\n",
    "    \n",
    "    # Build date filter\n",
    "    date_filter = \"\"\n",
    "    if year_from and year_to:\n",
    "        date_filter = f\" AND PUBYEAR > {year_from-1} AND PUBYEAR < {year_to+1}\"\n",
    "    elif year_from:\n",
    "        date_filter = f\" AND PUBYEAR > {year_from-1}\"\n",
    "    elif year_to:\n",
    "        date_filter = f\" AND PUBYEAR < {year_to+1}\"\n",
    "    \n",
    "    full_query = query + date_filter\n",
    "    \n",
    "    headers = {\n",
    "        'Accept': 'application/json',\n",
    "        'X-ELS-APIKey': api_key\n",
    "    }\n",
    "    \n",
    "    all_articles = []\n",
    "    start = 0\n",
    "    count_per_request = 25  # Scopus API limit\n",
    "    total_found = 0\n",
    "    \n",
    "    print(f\"🔄 Starting data collection for query...\")\n",
    "    \n",
    "    while len(all_articles) < max_records:\n",
    "        params = {\n",
    "            'query': full_query,\n",
    "            'count': count_per_request,\n",
    "            'start': start,\n",
    "            'view': 'STANDARD'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(base_url, params=params, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            if start == 0:  # First request\n",
    "                total_found = int(data.get('search-results', {}).get('opensearch:totalResults', 0))\n",
    "                print(f\"📊 Total results available: {total_found}\")\n",
    "            \n",
    "            articles = data.get('search-results', {}).get('entry', [])\n",
    "            \n",
    "            if not articles:\n",
    "                print(\"🔚 No more articles found - stopping collection\")\n",
    "                break\n",
    "                \n",
    "            all_articles.extend(articles)\n",
    "            start += count_per_request\n",
    "            \n",
    "            print(f\"📥 Collected {len(all_articles)} articles so far...\")\n",
    "            \n",
    "            # Check if we've reached the end\n",
    "            if len(articles) < count_per_request:\n",
    "                print(\"🔚 Reached end of available results\")\n",
    "                break\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"❌ API request failed at start={start}: {e}\")\n",
    "            break\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"❌ JSON parsing failed: {e}\")\n",
    "            break\n",
    "    \n",
    "    # Limit to max_records\n",
    "    if len(all_articles) > max_records:\n",
    "        all_articles = all_articles[:max_records]\n",
    "        print(f\"📋 Limited results to {max_records} articles as requested\")\n",
    "    \n",
    "    return total_found, all_articles\n",
    "\n",
    "def export_scopus_to_csv_and_update_summary(query, csv_folder, summary_folder, api_key, year_from, year_to, max_records=1000):\n",
    "    \"\"\"\n",
    "    Export Scopus results to CSV and update summary\n",
    "    \"\"\"\n",
    "    # Download articles\n",
    "    print(\"🚀 Starting Scopus data export...\")\n",
    "    total_found, articles = get_all_scopus_results(query, api_key, year_from, year_to, max_records=max_records)\n",
    "    actual_downloaded = len(articles)\n",
    "    \n",
    "    # Get next versioned CSV name and version number\n",
    "    csv_name, csv_path, version_number = get_next_versioned_filename(csv_folder, base_name=\"scopus_csv\", ext=\".csv\")\n",
    "    \n",
    "    print(f\"💾 Writing {actual_downloaded} articles to {csv_name}\")\n",
    "    \n",
    "    # Write to CSV\n",
    "    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['first_author', 'title', 'abstract', 'year', 'doi', 'journal', 'citation_count'])\n",
    "        \n",
    "        for article in articles:\n",
    "            # CORRECTED field extraction\n",
    "            first_author = extract_first_author(article.get('author'))\n",
    "            title = article.get('dc:title', '')\n",
    "            abstract = article.get('dc:description', '')  # This is correct for Scopus\n",
    "            year = article.get('prism:coverDate', '')[:4] if article.get('prism:coverDate') else ''\n",
    "            doi = article.get('prism:doi', '')\n",
    "            journal = article.get('prism:publicationName', '')\n",
    "            citation_count = article.get('citedby-count', '')\n",
    "            \n",
    "            writer.writerow([first_author, title, abstract, year, doi, journal, citation_count])\n",
    "    \n",
    "    print(f\"✅ Exported {actual_downloaded} records to {csv_path}\")\n",
    "\n",
    "    # Prepare versioned source name for summary row\n",
    "    source_name = f\"scopus v{version_number}\"\n",
    "\n",
    "    # Format timestamp as YYYY-MM-DDTHH:MM\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%dT%H:%M')\n",
    "\n",
    "    # Update summary CSV: always append, never overwrite, never repeat header\n",
    "    summary_csv_path = os.path.join(summary_folder, \"summary_csv.csv\")\n",
    "    file_exists = os.path.isfile(summary_csv_path)\n",
    "    \n",
    "    # Ensure file ends with a newline before appending\n",
    "    if file_exists and os.path.getsize(summary_csv_path) > 0:\n",
    "        ensure_newline_at_end(summary_csv_path)\n",
    "        \n",
    "    with open(summary_csv_path, 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not file_exists:\n",
    "            writer.writerow(['source', 'found', 'downloaded', 'keyword_combination', 'date'])\n",
    "        writer.writerow([source_name, total_found, actual_downloaded, query, timestamp])\n",
    "    \n",
    "    print(\"✅ Summary row updated.\")\n",
    "\n",
    "# Execute the export\n",
    "print(\"🎯 Starting final export with combined query...\")\n",
    "export_scopus_to_csv_and_update_summary(\n",
    "    combined_query, csv_folder, summary_folder, api_key, year_from, year_to, max_records=1000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d165c304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Starting final export with combined query...\n",
      "🚀 Starting Scopus data export...\n",
      "🔄 Starting data collection for query...\n",
      "📊 Total results available: 7\n",
      "📥 Collected 7 articles so far...\n",
      "🔚 Reached end of available results\n",
      "💾 Writing 7 articles to scopus_csv_v5.csv\n",
      "✅ Exported 7 records to /Users/petrakisherczegh/Documents/SP/csvs/scopus_csv/scopus_csv_v5.csv\n",
      "✅ Summary row updated.\n"
     ]
    }
   ],
   "source": [
    "# === SECTION: Download CSV and Update Summary (COMPLETED VIEW) ===\n",
    "\n",
    "def extract_first_author(article):\n",
    "    \"\"\"\n",
    "    Extract first author from Scopus dc:creator field\n",
    "    \"\"\"\n",
    "    # Scopus stores the first author in dc:creator field\n",
    "    dc_creator = article.get('dc:creator', '')\n",
    "    if dc_creator and isinstance(dc_creator, str):\n",
    "        return dc_creator\n",
    "    \n",
    "    # Fallback to author field if dc:creator is not available\n",
    "    authors_field = article.get('author', [])\n",
    "    if isinstance(authors_field, list) and authors_field:\n",
    "        first = authors_field[0]\n",
    "        if isinstance(first, dict):\n",
    "            return first.get('authname', '') or first.get('ce:indexed-name', '')\n",
    "    \n",
    "    return ''\n",
    "\n",
    "def get_next_versioned_filename(folder, base_name=\"scopus_csv\", ext=\".csv\"):\n",
    "    \"\"\"\n",
    "    Generate next available versioned filename\n",
    "    \"\"\"\n",
    "    i = 1\n",
    "    while True:\n",
    "        filename = f\"{base_name}_v{i}{ext}\"\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            return filename, filepath, i\n",
    "        i += 1\n",
    "\n",
    "def ensure_newline_at_end(filepath):\n",
    "    \"\"\"Ensures the file ends with a newline before appending.\"\"\"\n",
    "    if os.path.isfile(filepath):\n",
    "        with open(filepath, 'rb+') as f:\n",
    "            f.seek(-1, os.SEEK_END)\n",
    "            last_char = f.read(1)\n",
    "            if last_char != b'\\n':\n",
    "                f.write(b'\\n')\n",
    "\n",
    "def get_all_scopus_results(query, api_key, year_from, year_to, max_records=1000):\n",
    "    \"\"\"\n",
    "    Retrieve all results from Scopus API with pagination\n",
    "    \"\"\"\n",
    "    base_url = \"https://api.elsevier.com/content/search/scopus\"\n",
    "    \n",
    "    # Build date filter\n",
    "    date_filter = \"\"\n",
    "    if year_from and year_to:\n",
    "        date_filter = f\" AND PUBYEAR > {year_from-1} AND PUBYEAR < {year_to+1}\"\n",
    "    elif year_from:\n",
    "        date_filter = f\" AND PUBYEAR > {year_from-1}\"\n",
    "    elif year_to:\n",
    "        date_filter = f\" AND PUBYEAR < {year_to+1}\"\n",
    "    \n",
    "    full_query = query + date_filter\n",
    "    \n",
    "    headers = {\n",
    "        'Accept': 'application/json',\n",
    "        'X-ELS-APIKey': api_key\n",
    "    }\n",
    "    \n",
    "    all_articles = []\n",
    "    start = 0\n",
    "    count_per_request = 25  # Scopus API limit\n",
    "    total_found = 0\n",
    "    \n",
    "    print(f\"🔄 Starting data collection for query...\")\n",
    "    \n",
    "    while len(all_articles) < max_records:\n",
    "        params = {\n",
    "            'query': full_query,\n",
    "            'count': count_per_request,\n",
    "            'start': start,\n",
    "            # 'view': 'COMPLETE'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(base_url, params=params, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            if start == 0:  # First request\n",
    "                total_found = int(data.get('search-results', {}).get('opensearch:totalResults', 0))\n",
    "                print(f\"📊 Total results available: {total_found}\")\n",
    "            \n",
    "            articles = data.get('search-results', {}).get('entry', [])\n",
    "            \n",
    "            if not articles:\n",
    "                print(\"🔚 No more articles found - stopping collection\")\n",
    "                break\n",
    "                \n",
    "            all_articles.extend(articles)\n",
    "            start += count_per_request\n",
    "            \n",
    "            print(f\"📥 Collected {len(all_articles)} articles so far...\")\n",
    "            \n",
    "            # Check if we've reached the end\n",
    "            if len(articles) < count_per_request:\n",
    "                print(\"🔚 Reached end of available results\")\n",
    "                break\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"❌ API request failed at start={start}: {e}\")\n",
    "            break\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"❌ JSON parsing failed: {e}\")\n",
    "            break\n",
    "    \n",
    "    # Limit to max_records\n",
    "    if len(all_articles) > max_records:\n",
    "        all_articles = all_articles[:max_records]\n",
    "        print(f\"📋 Limited results to {max_records} articles as requested\")\n",
    "    \n",
    "    return total_found, all_articles\n",
    "\n",
    "def export_scopus_to_csv_and_update_summary(query, csv_folder, summary_folder, api_key, year_from, year_to, max_records=1000):\n",
    "    \"\"\"\n",
    "    Export Scopus results to CSV and update summary\n",
    "    \"\"\"\n",
    "    # Download articles\n",
    "    print(\"🚀 Starting Scopus data export...\")\n",
    "    total_found, articles = get_all_scopus_results(query, api_key, year_from, year_to, max_records=max_records)\n",
    "    actual_downloaded = len(articles)\n",
    "    \n",
    "    # Get next versioned CSV name and version number\n",
    "    csv_name, csv_path, version_number = get_next_versioned_filename(csv_folder, base_name=\"scopus_csv\", ext=\".csv\")\n",
    "    \n",
    "    print(f\"💾 Writing {actual_downloaded} articles to {csv_name}\")\n",
    "    \n",
    "    # Write to CSV\n",
    "    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['first_author', 'title', 'abstract', 'year', 'doi'])\n",
    "        \n",
    "        for article in articles:\n",
    "            # Use the corrected author extraction\n",
    "            first_author = extract_first_author(article)\n",
    "            title = article.get('dc:title', '')\n",
    "            abstract = article.get('dc:description', '')\n",
    "            year = article.get('prism:coverDate', '')[:4] if article.get('prism:coverDate') else ''\n",
    "            doi = article.get('prism:doi', '')\n",
    "            \n",
    "            writer.writerow([first_author, title, abstract, year, doi])\n",
    "    \n",
    "    print(f\"✅ Exported {actual_downloaded} records to {csv_path}\")\n",
    "\n",
    "    # Prepare versioned source name for summary row\n",
    "    source_name = f\"scopus v{version_number}\"\n",
    "\n",
    "    # Format timestamp as YYYY-MM-DDTHH:MM\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%dT%H:%M')\n",
    "\n",
    "    # Update summary CSV: always append, never overwrite, never repeat header\n",
    "    summary_csv_path = os.path.join(summary_folder, \"summary_csv.csv\")\n",
    "    file_exists = os.path.isfile(summary_csv_path)\n",
    "    \n",
    "    # Ensure file ends with a newline before appending\n",
    "    if file_exists and os.path.getsize(summary_csv_path) > 0:\n",
    "        ensure_newline_at_end(summary_csv_path)\n",
    "        \n",
    "    with open(summary_csv_path, 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not file_exists:\n",
    "            writer.writerow(['source', 'found', 'downloaded', 'keyword_combination', 'date'])\n",
    "        writer.writerow([source_name, total_found, actual_downloaded, query, timestamp])\n",
    "    \n",
    "    print(\"✅ Summary row updated.\")\n",
    "\n",
    "# Execute the export\n",
    "print(\"🎯 Starting final export with combined query...\")\n",
    "export_scopus_to_csv_and_update_summary(\n",
    "    combined_query, csv_folder, summary_folder, api_key, year_from, year_to, max_records=1000\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b33441",
   "metadata": {},
   "source": [
    "# Scopus Literature Search Strategy Completed\n",
    "\n",
    "If all scripts have been run successfully (either once or multiple times), you should've received confirmation messages for each block and have at least one csv named scopus_csv_v(n).csv in your folder defined at the start. Note, that with every single download the code generates an additional version following the naming convention of v1, v2, v3 etc. You should also have a summary table updated with a record of each download you made."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
